# MLGodotKit

**Empower your Godot projects with the power of machine learning!**  
MLGodotKit is a C++ GDExtension for Godot, enabling seamless integration of AI-driven features into your games and applications. With support for adaptive behaviors and real-time decision-making, it’s designed to inspire innovation and enhance gameplay.

<p align="center">
  <img src="MLGodotKit_logo.png" alt="MLGodotKit Logo" width="1000"/>
</p>


## 🎉 New Feature: WebAssembly Support

**WebAssembly (WASM) binaries** have been added to the project to enable web-based game development.  
You can now build and run your game directly in the browser using Godot's Web export.

**Play the Web Demo:**  
Try it out! Play as red and beat the (Ai) Blue. It starts off as fairly dumb but learns
to play pong and gets better the longer you play!
[Launch Game in Browser](https://fastidious-biscochitos-90f618.netlify.app)

📁 The exported web build is located at:  
`examples/NNPong/web/`

📘 Learn how to compile GDExtensions for the Web:  
[Godot Docs – Compiling for Web](https://docs.godotengine.org/en/stable/contributing/development/compiling/compiling_for_web.html#gdextension)


<p align="center">
  <img src="assets/pong-example.gif" alt="Pong Example" width="600"/>
</p>

---

## Node Extensions in Progress
- **Data Science Tooling**: Tools for preprocessing, analyzing, and visualizing data directly in Godot.
- **Deep Learning Integration**: Simplified nodes for building and deploying neural networks.
- **Cross-Platform Compatibility**: Fully supported on Windows, Linux, and macOS.

---

## LIMITATIONS
Please note that this is a work in process pet project of mine and as such will come with a few bugs. If you
enjoy the project and want to support please open an issue for desired features or bug fixes! 

# MLGodotKit Roadmap

### Completed
- [x] Linear Regression
- [x] Classification Decision Tree
- [x] Neural Network
- [x] Simple Activation Functions
    - [x] Linear
    - [x] Sigmoid
    - [x] ReLU

### Near Future
- [ ] Loss Functions (#8)
- [ ] Model Batch Training Support (#10)
- [ ] Local Model Store and Loading (#7)
- [ ] UI Training & Development Tools (#9)
- [ ] Model Evaluation Tools (#12)
- [ ] Testing Suite for all Nodes (#4)

### Documentation
- [ ] Training Tutorials (#11)
- [ ] API Reference

> *If you'd like to see something else add an issue!*

## Getting Started

Here are quick examples of how to use the three core models in **MLGodotKit**:

> *All models are GDExtension nodes and can be used directly in any Godot scene or script.*

---

## Linear Regression (`LRNode`)

```gdscript
extends Node

@onready var lr_model = LRNode.new()

func _ready():
	print("========== Test 1: Linear Regression Node ==========\n")
	lr_model.set_learning_rate(0.01)
	lr_model.initialize(1)  # Single feature input

	# Example dataset: y = 3x + 5
	var inputs = [[1], [2], [3], [4], [5]]
	var targets = [[8], [11], [14], [17], [20]]

	lr_model.train(inputs, targets, 1000)
	print()

	var all_pass = true  # Flag to track success/failure
	var tolerance = 1.0  # Acceptable error margin for predictions

	for i in range(inputs.size()):
		var pred_y = lr_model.predict(inputs[i])
		var actual_y = targets[i][0]
		var error = abs(pred_y[0] - actual_y)
		if error > tolerance:
			print("FAIL: Input: ", inputs[i], " Predicted: ", pred_y, " Actual: ", actual_y)
			all_pass = false
		else:
			print("PASS: Input: ", inputs[i], " Predicted: ", pred_y, " Actual: ", actual_y)

	if all_pass:
		print("Linear Regression test passed. \n")
	else:
		print("Linear Regression test failed. \n")
```

---

## Descision Tree Classifier (`DTreeNode`)

```gdscript
extends Node

@onready var tree = DTreeNode.new()

func _ready():
	print("========== Test 2: DTree Classification Node ==========\n")
	tree.set_min_samples_split(2)
	tree.set_max_depth(100)

	var X_train = [
		[1.0], [2.0], [3.0], [4.0], [5.0], [6.0], [7.0], [8.0], [9.0], [10.0],
		[11.0], [12.0], [13.0], [14.0], [15.0], [16.0], [17.0], [18.0], [19.0], [20.0]
	]
	var y_train = [
		0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
		1, 1, 1, 1, 1, 1, 1, 1, 1, 1
	]

	tree.fit(X_train, y_train)

	var X_test = [[2.0], [5.0], [10.0], [11.0], [15.0], [19.0]]
	var y_true = [0, 0, 0, 1, 1, 1]
	var predictions = tree.predict(X_test)

	var correct = 0
	for i in range(y_true.size()):
		if predictions[i] == y_true[i]:
			print("PASS: Input ", X_test[i], " Predicted: ", predictions[i], " Expected: ", y_true[i])
			correct += 1
		else:
			print("FAIL: Input ", X_test[i], " Predicted: ", predictions[i], " Expected: ", y_true[i])

	var accuracy = float(correct) / y_true.size()
	print("\nAccuracy: \n", accuracy)

	if accuracy >= 0.9:
		print("Decision Tree test passed. \n")
	else:
		print("Decision Tree test failed. \n")
```
---

## Neural Network (`NNNode`)

> *Note: All loss functions are left to the user to implement please see examples for suggested implementation
will be implemented natively at a later date.*

```gdscript
extends Node

@onready var nn_model = NNNode.new()

# Simple XOR dataset (non-linear, good for testing)
var inputs = [
	[0.0, 0.0],
	[0.0, 1.0],
	[1.0, 0.0],
	[1.0, 1.0]
]
var targets = [
	[0.0],  # 0 XOR 0 = 0
	[1.0],  # 0 XOR 1 = 1
	[1.0],  # 1 XOR 0 = 1
	[0.0]   # 1 XOR 1 = 0
]

# Hyperparameters
var epochs = 5000
var print_interval = epochs/10  # How often to print loss

func _ready():
	# Initialize NN model
	nn_model.set_verbosity(0)  # Set to 1 if you want to see all internal prints
	nn_model.set_learning_rate(0.1)
	
	# Add layers
	nn_model.add_layer(2, 4, "relu")
	nn_model.add_layer(4, 4, "relu")
	nn_model.add_layer(4, 1, "sigmoid")
	
	# Print model summary
	nn_model.model_summary()

	print("\nStarting training on XOR dataset...\n")

	# Training loop
	for epoch in range(1, epochs + 1):
		var total_loss = 0.0
		
		var indices = [0, 1, 2, 3]
		indices.shuffle()
		for i in indices:
			var x = [inputs[i]]
			var y_true = targets[i][0]
			# then process x and y_true as before

			
			# Forward pass
			var output = nn_model.forward(x)
			var y_pred = output[0] 
			
			# Compute error (difference)
			var error = y_pred - y_true
			
			# Compute simple squared loss
			var loss = error * error
			total_loss += loss
			
			# Backward pass (send gradient of loss w.r.t output)
			var grad = [ [2.0 * error] ]  # d(Loss)/d(Prediction)
			nn_model.backward(grad)
		
		# Print loss at intervals
		if epoch % print_interval == 0:
			print("Epoch ", epoch, " | Avg Loss: ", total_loss / inputs.size())
	
	print("\nTraining completed!\n")

	# Inference and Evaluation
	print("Testing trained model on XOR:")
	for i in range(inputs.size()):
		var x = [inputs[i]]
		var output = nn_model.forward(x)
		print("Input: ", x, " | Predicted: ", output, " | Expected: ", targets[i])

	# Auto-check: classify and compare
	var correct = 0
	for i in range(inputs.size()):
		var x = [inputs[i]]
		var output = nn_model.forward(x)
		var prediction = 1 if output[0] >= 0.5 else 0
		if prediction == int(targets[i][0]):
			correct += 1
	
	var accuracy = float(correct) / inputs.size()
	print("\n Final Accuracy: ", accuracy * 100.0, "%")

	if accuracy >= 0.75:  # We expect at least 75% on XOR
		print("NNNode passes XOR test!")
	else:
		print("NNNode failed XOR test. Needs improvement.")
	
	nn_model.model_summary()
```
---

## Credits
Built on the powerful [Eigen C++ library](https://eigen.tuxfamily.org/).


